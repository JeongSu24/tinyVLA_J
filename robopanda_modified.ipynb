{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20729b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "from llava_pythia.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava_pythia.mm_utils import tokenizer_image_token, get_model_name_from_path\n",
    "from llava_pythia.model.builder import load_pretrained_model\n",
    "from llava_pythia.conversation import conv_templates\n",
    "from llava_pythia.model.language_model.pythia.llava_pythia import LlavaPythiaConfig\n",
    "\n",
    "def get_image(ts, camera_names, rand_crop_resize=False):\n",
    "    curr_images = []\n",
    "    for cam_name in camera_names:\n",
    "        img = rearrange(ts.observation['images'][cam_name], 'h w c -> c h w')\n",
    "        curr_images.append(img)\n",
    "    curr_image = np.stack(curr_images, axis=0)\n",
    "    curr_image = torch.from_numpy(curr_image / 255.0).float().cuda().unsqueeze(0)\n",
    "\n",
    "    if rand_crop_resize:\n",
    "        print('rand crop resize is used!')\n",
    "        original_size = curr_image.shape[-2:]\n",
    "        ratio = 0.95\n",
    "        curr_image = curr_image[..., int(original_size[0]*(1-ratio)/2): int(original_size[0]*(1+ratio)/2),\n",
    "                                int(original_size[1]*(1-ratio)/2): int(original_size[1]*(1+ratio)/2)]\n",
    "        curr_image = curr_image.squeeze(0)\n",
    "        resize_transform = transforms.Resize(original_size, antialias=True)\n",
    "        curr_image = resize_transform(curr_image).unsqueeze(0)\n",
    "    return curr_image\n",
    "\n",
    "def convert_actions(pred_action):\n",
    "    return pred_action  # 그대로 반환\n",
    "\n",
    "class llava_pythia_act_policy:\n",
    "    def __init__(self, policy_config, data_args=None):\n",
    "        self.load_policy(policy_config)\n",
    "        self.data_args = data_args\n",
    "\n",
    "    def load_policy(self, policy_config):\n",
    "        self.policy_config = policy_config\n",
    "        model_base = policy_config[\"model_base\"] if policy_config['enable_lora'] else None\n",
    "        model_name = get_model_name_from_path(policy_config['model_path'])\n",
    "        model_path = policy_config[\"model_path\"]\n",
    "\n",
    "        self.tokenizer, self.policy, self.image_processor, self.context_len = load_pretrained_model(\n",
    "            model_path, model_base, model_name, False, False\n",
    "        )\n",
    "        self.config = LlavaPythiaConfig.from_pretrained(\n",
    "            '/'.join(model_path.split('/')[:-1]),\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    def process_batch_to_llava(self, curr_image, robo_state, raw_lang):\n",
    "        self.conv = conv_templates[self.policy_config['conv_mode']].copy()\n",
    "\n",
    "        if len(curr_image.shape) == 5:\n",
    "            curr_image = curr_image.squeeze(0)\n",
    "\n",
    "        image, image_r = torch.chunk(curr_image, 2, dim=0)\n",
    "\n",
    "        image = self.expand2square(image, tuple(x for x in self.image_processor.image_mean))\n",
    "        image_tensor = self.image_processor.preprocess(\n",
    "            image, return_tensors='pt', do_normalize=True, do_rescale=False, do_center_crop=False\n",
    "        )['pixel_values'].to(torch.float32).to(self.policy.device)\n",
    "\n",
    "        image_r = self.expand2square(image_r, tuple(x for x in self.image_processor.image_mean))\n",
    "        image_tensor_r = self.image_processor.preprocess(\n",
    "            image_r, return_tensors='pt', do_normalize=True, do_rescale=False, do_center_crop=False\n",
    "        )['pixel_values'].to(torch.float32).to(self.policy.device)\n",
    "\n",
    "        inp = raw_lang\n",
    "        if self.policy.config.mm_use_im_start_end:\n",
    "            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "        else:\n",
    "            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "\n",
    "        self.conv.append_message(self.conv.roles[0], inp)\n",
    "        self.conv.append_message(self.conv.roles[1], None)\n",
    "        prompt = self.conv.get_prompt() + \" <|endoftext|>\"\n",
    "\n",
    "        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX,\n",
    "                                          return_tensors='pt').unsqueeze(0).cuda().long()\n",
    "        attn_mask = input_ids.ne(self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        states = robo_state.to(self.policy.device, dtype=torch.float32)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            images=image_tensor.float(),\n",
    "            images_r=image_tensor_r.float(),\n",
    "            # states=states  # 필요 시 추가\n",
    "        )\n",
    "\n",
    "    def expand2square(self, pil_imgs, background_color):\n",
    "        batch_size, channels, height, width = pil_imgs.shape\n",
    "        max_dim = max(height, width)\n",
    "        expanded_imgs = np.full((batch_size, max_dim, max_dim, channels),\n",
    "                                background_color, dtype=np.float32)\n",
    "\n",
    "        if height == width:\n",
    "            expanded_imgs = pil_imgs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        elif height > width:\n",
    "            offset = (max_dim - width) // 2\n",
    "            expanded_imgs[:, :height, offset:offset+width, :] = pil_imgs.permute(0,2,3,1).cpu().numpy()\n",
    "        else:\n",
    "            offset = (max_dim - height) // 2\n",
    "            expanded_imgs[:, offset:offset+height, :width, :] = pil_imgs.permute(0,2,3,1).cpu().numpy()\n",
    "\n",
    "        return torch.tensor(expanded_imgs).to(dtype=pil_imgs.dtype, device=pil_imgs.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "        )\n",
    "\n",
    "    def process_batch_to_llava(self, curr_image, robo_state, raw_lang):\n",
    "        self.conv = conv_templates[self.policy_config['conv_mode']].copy()\n",
    "\n",
    "        if len(curr_image.shape) == 5:\n",
    "            curr_image = curr_image.squeeze(0)\n",
    "\n",
    "        image, image_r = torch.chunk(curr_image, 2, dim=0)\n",
    "\n",
    "        image = self.expand2square(image, tuple(x for x in self.image_processor.image_mean))\n",
    "        image_tensor = self.image_processor.preprocess(\n",
    "            image, return_tensors='pt', do_normalize=True, do_rescale=False, do_center_crop=False\n",
    "        )['pixel_values'].to(torch.float32).to(self.policy.device)\n",
    "\n",
    "        image_r = self.expand2square(image_r, tuple(x for x in self.image_processor.image_mean))\n",
    "        image_tensor_r = self.image_processor.preprocess(\n",
    "            image_r, return_tensors='pt', do_normalize=True, do_rescale=False, do_center_crop=False\n",
    "        )['pixel_values'].to(torch.float32).to(self.policy.device)\n",
    "\n",
    "        inp = raw_lang\n",
    "        if self.policy.config.mm_use_im_start_end:\n",
    "            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "        else:\n",
    "            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "\n",
    "        self.conv.append_message(self.conv.roles[0], inp)\n",
    "        self.conv.append_message(self.conv.roles[1], None)\n",
    "        prompt = self.conv.get_prompt() + \" <|endoftext|>\"\n",
    "\n",
    "        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX,\n",
    "                                          return_tensors='pt').unsqueeze(0).cuda().long()\n",
    "        attn_mask = input_ids.ne(self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        states = robo_state.to(self.policy.device, dtype=torch.float32)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            images=image_tensor.float(),\n",
    "            images_r=image_tensor_r.float(),\n",
    "            # states=states  # 필요 시 추가\n",
    "        )\n",
    "\n",
    "    def expand2square(self, pil_imgs, background_color):\n",
    "        batch_size, channels, height, width = pil_imgs.shape\n",
    "        max_dim = max(height, width)\n",
    "        expanded_imgs = np.full((batch_size, max_dim, max_dim, channels),\n",
    "                                background_color, dtype=np.float32)\n",
    "\n",
    "        if height == width:\n",
    "            expanded_imgs = pil_imgs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        elif height > width:\n",
    "            offset = (max_dim - width) // 2\n",
    "            expanded_imgs[:, :height, offset:offset+width, :] = pil_imgs.permute(0,2,3,1).cpu().numpy()\n",
    "        else:\n",
    "            offset = (max_dim - height) // 2\n",
    "            expanded_imgs[:, offset:offset+height, :width, :] = pil_imgs.permute(0,2,3,1).cpu().numpy()\n",
    "\n",
    "        return torch.tensor(expanded_imgs).to(dtype=pil_imgs.dtype, device=pil_imgs.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5364f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.controllers import load_composite_controller_config\n",
    "import numpy as np\n",
    "\n",
    "class RobosuiteDeployEnv:\n",
    "    def __init__(self, env_name=\"Lift\", cameras=(\"sideview\", \"frontview\"),\n",
    "                 control_freq=20):\n",
    "        \"\"\"\n",
    "        DROID 스타일 시점으로 카메라를 재배치한 robosuite 환경\n",
    "        기존 카메라 이름만 사용하고, 위치/각도를 덮어써서 DROID 느낌으로 변환\n",
    "        \"\"\"\n",
    "        controller_config = load_composite_controller_config(robot=\"Panda\")\n",
    "\n",
    "        # ✅ robosuite 환경 생성\n",
    "        self.env = suite.make(\n",
    "            env_name=env_name,\n",
    "            robots=\"Panda\",\n",
    "            controller_configs=controller_config,\n",
    "            has_renderer=False,\n",
    "            has_offscreen_renderer=True,\n",
    "            render_camera=None,\n",
    "            use_object_obs=True,\n",
    "            use_camera_obs=True,\n",
    "            control_freq=control_freq,\n",
    "            camera_names=list(cameras),   # 기존 등록된 카메라만 사용\n",
    "            camera_heights=240,\n",
    "            camera_widths=320,\n",
    "        )\n",
    "\n",
    "        # ✅ 카메라 위치/각도 재배치 (DROID 스타일)\n",
    "        sim = self.env.sim\n",
    "\n",
    "        # sideview → DROID 왼쪽 카메라처럼 설정\n",
    "        # ✅ sideview → 왼쪽 위, 사선 내려보기\n",
    "        cam_id_side = sim.model.camera_name2id(\"sideview\")\n",
    "        sim.model.cam_pos[cam_id_side] = [0.4, 0.8, 1.0]\n",
    "        sim.model.cam_quat[cam_id_side] = [0.653, 0.271, -0.653, 0.271]\n",
    "\n",
    "        # ✅ frontview → 오른쪽 위, 사선 내려보기\n",
    "        cam_id_front = sim.model.camera_name2id(\"frontview\")\n",
    "        sim.model.cam_pos[cam_id_front] = [-0.4, -0.8, 1.0]\n",
    "        sim.model.cam_quat[cam_id_front] = [0.653, -0.271, 0.653, 0.271]\n",
    "\n",
    "\n",
    "        # ✅ 초기 reset\n",
    "        self.obs = self.env.reset()\n",
    "        print(\"DEBUG obs keys:\", self.obs.keys())\n",
    "        print(\"✅ 카메라 시점 재배치 완료! 현재 카메라:\", self.env.camera_names)\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        ✅ 현재 step의 관측값을 (ts, robot_state) 형태로 반환\n",
    "        \"\"\"\n",
    "        obs = self.obs\n",
    "\n",
    "        # 1) 이미지 dictionary 생성\n",
    "        ts = type(\"Timestep\", (), {})()\n",
    "        ts.observation = {'images': {}}\n",
    "        for cam_name in self.env.camera_names:\n",
    "            ts.observation['images'][cam_name] = obs[f'{cam_name}_image']\n",
    "\n",
    "        # 2) 로봇 상태 (eef + gripper)\n",
    "        eef_pos = obs['robot0_eef_pos']\n",
    "        eef_quat = obs['robot0_eef_quat']\n",
    "        gripper_qpos = obs['robot0_gripper_qpos']\n",
    "        gripper_state = np.mean(gripper_qpos)\n",
    "\n",
    "        robot_state = np.concatenate([eef_pos, eef_quat, [gripper_state]])\n",
    "\n",
    "        return ts, robot_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return self.obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.obs, reward, done, info = self.env.step(action)\n",
    "        return self.obs, reward, done, info\n",
    "\n",
    "    def render_cameras(self, cameras=(\"droid_left\", \"droid_right\"), width=320, height=240):\n",
    "        frames = []\n",
    "        # ⚠️ Mujoco 내부 상태 업데이트\n",
    "        self.env.sim.forward()\n",
    "\n",
    "        for cam in cameras:\n",
    "            frame = self.env.sim.render(\n",
    "                camera_name=cam,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                depth=False,        # ✅ depth buffer 강제 OFF\n",
    "                mode=\"offscreen\",   # ✅ GPU offscreen 렌더링\n",
    "            )\n",
    "            # ⚠️ robosuite 기본은 RGB이지만, 일부 드라이버에서 BGR 출력됨 → 변환\n",
    "            if frame.shape[-1] == 3:\n",
    "                frame = frame[..., ::-1]\n",
    "            frames.append(frame)\n",
    "        return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5164729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def eval_bc(policy, deploy_env, policy_config, save_episode=True, num_rollouts=1,\n",
    "            raw_lang=None, n_steps=50, fps=20,\n",
    "            camera_names=(\"sideview\", \"frontview\")):   # ✅ 기본 카메라를 DROID 스타일로 변경\n",
    "    assert raw_lang is not None, \"raw lang is None!!!!!!\"\n",
    "\n",
    "    all_frames = []\n",
    "\n",
    "    for rollout_idx in range(num_rollouts):\n",
    "        print(f\"=== rollout {rollout_idx} start ===\")\n",
    "        deploy_env.reset()\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            # ✅ 새로운 카메라 이름으로 get_observation\n",
    "            ts, robot_state = deploy_env.get_observation()\n",
    "            traj_rgb_np = get_image(ts, camera_names)   # ✅ 여기서도 바꿔줌\n",
    "\n",
    "            robot_state_tensor = torch.from_numpy(robot_state).float().cuda()\n",
    "\n",
    "            batch = policy.process_batch_to_llava(traj_rgb_np, robot_state_tensor, raw_lang)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                all_actions = policy.policy(**batch, eval=True)\n",
    "            print(\"=== DEBUG: all_actions shape:\", all_actions.shape)\n",
    "            print(\"=== DEBUG: first all_actions[0]:\", all_actions[0])\n",
    "\n",
    "            raw_action = all_actions[0][0].detach().cpu().numpy()\n",
    "            action = convert_actions(raw_action)\n",
    "\n",
    "            obs, reward, done, info = deploy_env.step(action)\n",
    "            print(f\"[t={t}] reward={reward:.4f}, done={done}\")\n",
    "\n",
    "            # ✅ 여기서도 새로운 카메라 시점 사용\n",
    "            frames = deploy_env.render_cameras(\n",
    "                cameras=camera_names,   # ✅ 바꾼 카메라 반영\n",
    "                width=640, height=480\n",
    "            )\n",
    "            merged_frame = np.concatenate(frames, axis=1)\n",
    "            all_frames.append(merged_frame)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # ✅ mp4 저장 부분은 동일\n",
    "    if save_episode and all_frames:\n",
    "        h, w, _ = all_frames[0].shape\n",
    "        out = cv2.VideoWriter(\"rollout.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "        for frame in all_frames:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "        print(\"✅ rollout.mp4 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46185e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load llaVA-Pythia MLLM!!!\n",
      "number of parameters: 7.283150e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: /home/parkjeongsu/anaconda3/envs/tinysuite/lib/python3.10/site-packages/robosuite/controllers/config/robots/default_panda.json (composite_controller_factory.py:121)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device_map': 'cuda', 'torch_dtype': torch.float32}\n",
      "DEBUG obs keys: odict_keys(['robot0_joint_pos', 'robot0_joint_pos_cos', 'robot0_joint_pos_sin', 'robot0_joint_vel', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_eef_quat_site', 'robot0_gripper_qpos', 'robot0_gripper_qvel', 'sideview_image', 'frontview_image', 'cube_pos', 'cube_quat', 'gripper_to_cube_pos', 'robot0_proprio-state', 'object-state'])\n",
      "✅ 카메라 시점 재배치 완료! 현재 카메라: ['sideview', 'frontview']\n",
      "=== rollout 0 start ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x512 and 519x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m policy \u001b[38;5;241m=\u001b[39m llava_pythia_act_policy(policy_config)\n\u001b[1;32m     16\u001b[0m deploy_env \u001b[38;5;241m=\u001b[39m RobosuiteDeployEnv(\n\u001b[1;32m     17\u001b[0m     env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLift\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     cameras\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msideview\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrontview\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43meval_bc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeploy_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rollouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m, in \u001b[0;36meval_bc\u001b[0;34m(policy, deploy_env, policy_config, save_episode, num_rollouts, raw_lang, n_steps, fps, camera_names)\u001b[0m\n\u001b[1;32m     22\u001b[0m batch \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mprocess_batch_to_llava(traj_rgb_np, robot_state_tensor, raw_lang)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     all_actions \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== DEBUG: all_actions shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_actions\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== DEBUG: first all_actions[0]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_actions[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tinysuite/lib/python3.10/site-packages/llava_pythia/model/language_model/pythia/llava_pythia.py:228\u001b[0m, in \u001b[0;36mLlavaPythiaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, return_dict, actions, images_r, images_top, is_pad, eval)\u001b[0m\n\u001b[1;32m    226\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_diffusion_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_pad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m action\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# return outputs as a tuple instead of a structured dictionary\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tinysuite/lib/python3.10/site-packages/llava_pythia/model/language_model/pythia/llava_pythia.py:397\u001b[0m, in \u001b[0;36mLlavaPythiaForCausalLM.forward_diffusion_head\u001b[0;34m(self, actions, hidden_states, is_pad)\u001b[0m\n\u001b[1;32m    394\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(k) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(k, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# predict noise (embed_out expects float32)\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnaction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# inverse diffusion step (remove noise)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m naction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_scheduler\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    401\u001b[0m     model_output\u001b[38;5;241m=\u001b[39mnoise_pred,\n\u001b[1;32m    402\u001b[0m     timestep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(k), \u001b[38;5;66;03m# ✅ float 또는 Half → int로 캐스팅\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     sample\u001b[38;5;241m=\u001b[39mnaction\n\u001b[1;32m    404\u001b[0m )\u001b[38;5;241m.\u001b[39mprev_sample\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/TinyVLA/policy_heads/models/droid_unet_diffusion.py:270\u001b[0m, in \u001b[0;36mConditionalUnet1D.forward\u001b[0;34m(self, sample, timestep, global_cond, states)\u001b[0m\n\u001b[1;32m    268\u001b[0m global_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_after_pool(global_cond) \u001b[38;5;66;03m# layernorm\u001b[39;00m\n\u001b[1;32m    269\u001b[0m global_cond \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([global_cond, states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m global_cond\n\u001b[0;32m--> 270\u001b[0m global_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m timestep\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(timesteps):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x512 and 519x512)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    policy_config = {\n",
    "        \"model_path\": \"/home/parkjeongsu/TinyVLA/OUTPUT_llava_pythia/checkpoint-2000\",\n",
    "        \"model_base\": \"/home/parkjeongsu/TinyVLA/Llava-Pythia-400M\",\n",
    "        \"enable_lora\": False,\n",
    "        \"conv_mode\": \"pythia\",\n",
    "        \"action_head\": \"droid_diffusion\",\n",
    "        \"action_dim\": 7,\n",
    "        \"chunk_size\": 10\n",
    "    }\n",
    "\n",
    "    raw_lang = \"pick up the cube and place it on the table\"\n",
    "\n",
    "    policy = llava_pythia_act_policy(policy_config)\n",
    "\n",
    "    deploy_env = RobosuiteDeployEnv(\n",
    "        env_name=\"Lift\",\n",
    "        cameras=(\"sideview\", \"frontview\")\n",
    "    )\n",
    "\n",
    "    eval_bc(\n",
    "        policy,\n",
    "        deploy_env,\n",
    "        policy_config,\n",
    "        save_episode=True,\n",
    "        num_rollouts=1,\n",
    "        raw_lang=raw_lang,\n",
    "        n_steps=30,\n",
    "        fps=20\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c22d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left', 'right']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"/home/parkjeongsu/TinyVLA/droid_with_lang/droid_1dot7t_lang_succ_t0001_s-0-0/episode_171777.hdf5\", \"r\") as f:\n",
    "    print(list(f['observations/images'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2e4e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language_raw: [b'Press a button on the keyboard']\n",
      "action: (304, 10)\n",
      "qpos: (304, 7)\n",
      "qvel: (304, 7)\n",
      "images/left: (304, 180, 320, 3)\n",
      "images/right: (304, 180, 320, 3)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unable to synchronously open object (object 'joint_position' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, imgs[k]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoint_positions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoint_position\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m \u001b[43minspect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/parkjeongsu/TinyVLA/droid_with_lang/droid_1dot7t_lang_succ_t0001_s-0-0/episode_5392544.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36minspect\u001b[0;34m(h5_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m imgs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, imgs[k]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoint_positions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjoint_position\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tinysuite/lib/python3.10/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:241\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to synchronously open object (object 'joint_position' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "def inspect(h5_path):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        print(\"language_raw:\", f[\"language_raw\"][...])\n",
    "        print(\"action:\", f[\"action\"].shape)  # (T,10)\n",
    "        obs = f[\"observations\"]\n",
    "        print(\"qpos:\", obs[\"qpos\"].shape)\n",
    "        print(\"qvel:\", obs[\"qvel\"].shape)\n",
    "        imgs = obs[\"images\"]\n",
    "        for k in imgs.keys():\n",
    "            print(f\"images/{k}:\", imgs[k].shape)\n",
    "        print(\"joint_positions:\", obs[\"joint_position\"].shape)\n",
    "inspect(\"/home/parkjeongsu/TinyVLA/droid_with_lang/droid_1dot7t_lang_succ_t0001_s-0-0/episode_5392544.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebb5dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: /home/parkjeongsu/anaconda3/envs/tinysuite/lib/python3.10/site-packages/robosuite/controllers/config/robots/default_panda.json (composite_controller_factory.py:121)\n",
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: /home/parkjeongsu/anaconda3/envs/tinysuite/lib/python3.10/site-packages/robosuite/controllers/config/robots/default_panda.json (composite_controller_factory.py:121)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3) (480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import robosuite as suite\n",
    "\n",
    "env = suite.make(\n",
    "    \"Lift\",\n",
    "    robots=\"Panda\",\n",
    "    has_renderer=False,\n",
    "    use_camera_obs=True,\n",
    "    camera_names=[\"sideview\", \"frontview\"],  # 기존 카메라 이름 사용\n",
    "    camera_heights=480,\n",
    "    camera_widths=640,\n",
    ")\n",
    "\n",
    "sim = env.sim\n",
    "\n",
    "# sideview → DROID 왼쪽 느낌\n",
    "cam_id_side = sim.model.camera_name2id(\"sideview\")\n",
    "sim.model.cam_pos[cam_id_side] = [1.2, 0.5, 1.4]\n",
    "sim.model.cam_quat[cam_id_side] = [0.7, 0.0, -0.7, 0.0]\n",
    "\n",
    "# frontview → DROID 오른쪽 느낌\n",
    "cam_id_front = sim.model.camera_name2id(\"frontview\")\n",
    "sim.model.cam_pos[cam_id_front] = [-1.2, -0.5, 1.4]\n",
    "sim.model.cam_quat[cam_id_front] = [0.7, 0.0, 0.7, 0.0]\n",
    "\n",
    "obs = env.reset()\n",
    "left_img = obs[\"sideview_image\"]\n",
    "right_img = obs[\"frontview_image\"]\n",
    "print(left_img.shape, right_img.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinysuite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
