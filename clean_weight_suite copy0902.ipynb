{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c532f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "import imageio\n",
    "\n",
    "# llava_pythia / robosuite\n",
    "from llava_pythia.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava_pythia.mm_utils import tokenizer_image_token, get_model_name_from_path\n",
    "from llava_pythia.model.builder import load_pretrained_model\n",
    "from llava_pythia.conversation import conv_templates\n",
    "from llava_pythia.model.language_model.pythia.llava_pythia import LlavaPythiaConfig\n",
    "\n",
    "import robosuite as suite\n",
    "from robosuite.controllers import load_composite_controller_config\n",
    "from robosuite.utils.transform_utils import (\n",
    "    mat2quat, quat_multiply, quat_inverse, quat2axisangle\n",
    ")\n",
    "\n",
    "# ===================== ì´ë¯¸ì§€ ì¤€ë¹„ =====================\n",
    "def get_image(ts, camera_names, rand_crop_resize=False):\n",
    "    # í•™ìŠµ ë¶„í¬ì™€ ë™ì¼í•˜ê²Œ 180x320\n",
    "    imgs = []\n",
    "    for cam in camera_names:\n",
    "        im = ts.observation['images'][cam]\n",
    "        if (im.shape[0], im.shape[1]) != (180, 320):\n",
    "            im = cv2.resize(im, (320, 180))\n",
    "        imgs.append(rearrange(im, 'h w c -> c h w'))\n",
    "    img_tensor = torch.from_numpy(np.stack(imgs) / 255.0).float().cuda().unsqueeze(0)\n",
    "\n",
    "    if rand_crop_resize:\n",
    "        h, w = img_tensor.shape[-2:]\n",
    "        ratio = 0.95\n",
    "        dh, dw = int(h * (1 - ratio) / 2), int(w * (1 - ratio) / 2)\n",
    "        img_tensor = img_tensor[..., dh:h - dh, dw:w - dw].squeeze(0)\n",
    "        img_tensor = transforms.Resize((h, w), antialias=True)(img_tensor).unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "# ===================== íšŒì „ ìœ í‹¸ (6D -> R, R -> quat wxyz) =====================\n",
    "def rot6d_to_matrix(rot6d: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    rot6d: (..., 6) -> rotation matrix (..., 3, 3)\n",
    "    \"\"\"\n",
    "    a1 = torch.nn.functional.normalize(rot6d[..., 0:3], dim=-1)\n",
    "    a2 = rot6d[..., 3:6]\n",
    "    b2 = torch.nn.functional.normalize(a2 - (a1 * a2).sum(-1, keepdim=True) * a1, dim=-1)\n",
    "    b3 = torch.cross(a1, b2, dim=-1)\n",
    "    return torch.stack([a1, b2, b3], dim=-2)  # (...,3,3)\n",
    "\n",
    "def matrix_to_quat_xyzw(R: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    torch (...,3,3) -> numpy (...,4) in (x,y,z,w)\n",
    "    mat2quatëŠ” (w,x,y,z)ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ xyzwë¡œ ì¬ì •ë ¬\n",
    "    \"\"\"\n",
    "    R_np = R.detach().cpu().numpy()\n",
    "    q_wxyz = np.asarray([mat2quat(r) for r in R_np])      # (N,4) [w,x,y,z]\n",
    "    q_xyzw = np.concatenate([q_wxyz[:, 1:4], q_wxyz[:, 0:1]], axis=-1)\n",
    "    return q_xyzw\n",
    "\n",
    "\n",
    "\n",
    "# ===================== ì •ì±… ë˜í¼ =====================\n",
    "class llava_pythia_act_policy:\n",
    "    def __init__(self, policy_config, data_args=None):\n",
    "        self.policy_config = policy_config\n",
    "        self.data_args = data_args\n",
    "        self._load_policy()\n",
    "\n",
    "    def _load_policy(self):\n",
    "        base = self.policy_config[\"model_base\"] if self.policy_config['enable_lora'] else None\n",
    "        name = get_model_name_from_path(self.policy_config['model_path'])\n",
    "        path = self.policy_config[\"model_path\"]\n",
    "        self.tokenizer, self.policy, self.image_processor, self.context_len = load_pretrained_model(\n",
    "            path, base, name, False, False\n",
    "        )\n",
    "        self.config = LlavaPythiaConfig.from_pretrained('/'.join(path.split('/')[:-1]), trust_remote_code=True)\n",
    "\n",
    "    def _expand2square(self, imgs, bg_color):\n",
    "        b, c, h, w = imgs.shape\n",
    "        size = max(h, w)\n",
    "        canvas = np.full((b, size, size, c), bg_color, dtype=np.float32)\n",
    "        imgs_np = imgs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        if h >= w:\n",
    "            offset = (size - w) // 2\n",
    "            canvas[:, :h, offset:offset + w, :] = imgs_np\n",
    "        else:\n",
    "            offset = (size - h) // 2\n",
    "            canvas[:, offset:offset + h, :w, :] = imgs_np\n",
    "        return torch.tensor(canvas).to(dtype=imgs.dtype, device=imgs.device)\n",
    "\n",
    "    def process_batch_to_llava(self, curr_image, robo_state, raw_lang):\n",
    "        self.conv = conv_templates[self.policy_config['conv_mode']].copy()\n",
    "        curr_image = curr_image.squeeze(0) if curr_image.dim() == 5 else curr_image\n",
    "        img1, img2, img3 = torch.chunk(curr_image, 3, dim=0)\n",
    "        states = robo_state.unsqueeze(0) if robo_state.dim() == 1 else robo_state\n",
    "\n",
    "        def prep(img):\n",
    "            img = self._expand2square(img, tuple(self.image_processor.image_mean))\n",
    "            return self.image_processor.preprocess(\n",
    "                img, return_tensors='pt', do_normalize=True, do_rescale=False, do_center_crop=False\n",
    "            )['pixel_values'].float().to(self.policy.device)\n",
    "\n",
    "        image_tensor     = prep(img1)\n",
    "        image_tensor_r   = prep(img2)\n",
    "        image_tensor_top = prep(img3)\n",
    "\n",
    "        prompt = DEFAULT_IMAGE_TOKEN + '\\n' + raw_lang\n",
    "        if self.policy.config.mm_use_im_start_end:\n",
    "            prompt = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + raw_lang\n",
    "        self.conv.append_message(self.conv.roles[0], prompt)\n",
    "        self.conv.append_message(self.conv.roles[1], None)\n",
    "        prompt = self.conv.get_prompt() + \" <|endoftext|>\"\n",
    "\n",
    "        input_ids = tokenizer_image_token(\n",
    "            prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'\n",
    "        ).unsqueeze(0).cuda().long()\n",
    "        attn_mask = input_ids.ne(self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids, attention_mask=attn_mask,\n",
    "            images=image_tensor, images_r=image_tensor_r, images_top=image_tensor_top,\n",
    "            states=states\n",
    "        )\n",
    "\n",
    "# ===================== ë°°ì¹˜ í™˜ê²½ =====================\n",
    "class RobosuiteDeployEnv:\n",
    "    def __init__(self, env_name=\"Lift\", cameras=(\"sideview\", \"sideview_opposite\", \"robot0_eye_in_hand\"), control_freq=20):\n",
    "        controller_config = load_composite_controller_config(robot=\"Panda\")\n",
    "        self.env = suite.make(\n",
    "            env_name=env_name, robots=\"Panda\", controller_configs=controller_config,\n",
    "            has_renderer=False, has_offscreen_renderer=True, render_camera=None,\n",
    "            use_object_obs=True, use_camera_obs=True, control_freq=control_freq,\n",
    "            camera_names=list(cameras), camera_heights=240, camera_widths=320\n",
    "        )\n",
    "        self.view_map = {cam: cam for cam in cameras}\n",
    "        self.logical_camera_names = cameras\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return self.obs\n",
    "\n",
    "    def get_observation(self):\n",
    "        ts = type(\"Timestep\", (), {})()\n",
    "        ts.observation = {'images': {cam: self.obs[f'{cam}_image'] for cam in self.env.camera_names}}\n",
    "\n",
    "        # ê´€ì ˆ 7Dë¥¼ stateë¡œ (í•™ìŠµê³¼ ë™ì¼)\n",
    "        if 'robot0_joint_pos' in self.obs:\n",
    "            qpos7 = self.obs['robot0_joint_pos'][:7].copy()\n",
    "        else:\n",
    "            qpos7 = self.env.robots[0]._joint_positions[:7].copy()\n",
    "\n",
    "        return ts, qpos7\n",
    "\n",
    "    def step(self, action):\n",
    "        self.obs, reward, done, info = self.env.step(action)\n",
    "        return self.obs, reward, done, info\n",
    "\n",
    "    def render_cameras(self, cameras, width=320, height=240):\n",
    "        self.env.sim.forward()\n",
    "        mujoco_cam_names = [self.view_map[cam_name] for cam_name in cameras]\n",
    "        images = []\n",
    "        for c in mujoco_cam_names:\n",
    "            img = self.env.sim.render(camera_name=c, width=width, height=height, depth=False)\n",
    "            images.append(np.flipud(img))\n",
    "        return images\n",
    "\n",
    "# ===================== í‰ê°€ ë£¨í”„ =====================\n",
    "def eval_bc(\n",
    "    policy, deploy_env, policy_config, save_episode=True, num_rollouts=1,\n",
    "    raw_lang=None, n_steps=50, fps=20,\n",
    "    camera_names=(\"sideview\", \"sideview_opposite\", \"robot0_eye_in_hand\")\n",
    "):\n",
    "    assert raw_lang is not None\n",
    "    all_frames = []\n",
    "\n",
    "    # ckpt ì„¤ì •\n",
    "    ckpt_action_dim = getattr(policy.policy.config, \"action_dim\", 7)\n",
    "    state_dim       = getattr(policy.policy.config, \"state_dim\", 7)\n",
    "    use_state_ckpt  = bool(getattr(policy.policy.config, \"use_state\", False))\n",
    "    head_type       = getattr(policy.policy.config, \"action_head_type\", \"\")\n",
    "    print(f\"[ckpt] action_dim={ckpt_action_dim}, state_dim={state_dim}, use_state={use_state_ckpt}, head={head_type}\")\n",
    "\n",
    "    # diffusionì´ë©´ ì—­ì •ê·œí™” ì¤€ë¹„\n",
    "    import os, pickle\n",
    "    stats_path = os.path.join(os.path.dirname(policy_config[\"model_path\"]), \"dataset_stats.pkl\")\n",
    "    amin = amax = None\n",
    "    if head_type == \"droid_diffusion\":\n",
    "        with open(stats_path, \"rb\") as f:\n",
    "            stats = pickle.load(f)\n",
    "        amin = torch.tensor(stats[\"action_min\"], device=\"cuda\", dtype=torch.float32)\n",
    "        amax = torch.tensor(stats[\"action_max\"], device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "        def denorm_action(a_norm: torch.Tensor) -> torch.Tensor:\n",
    "            return (a_norm + 1.0) * 0.5 * (amax - amin) + amin\n",
    "\n",
    "    # ì œì–´ ê²Œì¸ / í´ë¨í”„\n",
    "    POS_GAIN = 0.2\n",
    "    ROT_GAIN = 0.2\n",
    "    POS_CLIP = 0.03\n",
    "    ROT_CLIP = 0.25\n",
    "\n",
    "    # === ê·¸ë¦¬í¼ íŒŒë¼ë¯¸í„° (ë¡œì»¬ ë³€ìˆ˜ë¡œ ìœ ì§€) ===\n",
    "    GRIP_VEL_CLIP = 0.5\n",
    "    G_GAIN = 2.0\n",
    "    GRIP_EMA_ALPHA = 0.2\n",
    "    grip_ema = None       # â† ì „ì—­(global) ì“°ì§€ ë§ê³  í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìœ ì§€\n",
    "\n",
    "    for rollout_idx in range(num_rollouts):\n",
    "        deploy_env.reset()\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            ts, robot_state = deploy_env.get_observation()\n",
    "\n",
    "            # ì¹´ë©”ë¼ ë§¤í•‘\n",
    "            mapped_obs_images = {\n",
    "                logical_name: ts.observation['images'][mujoco_name]\n",
    "                for logical_name, mujoco_name in deploy_env.view_map.items()\n",
    "                if mujoco_name in ts.observation['images']\n",
    "            }\n",
    "            ts.observation['images'] = mapped_obs_images\n",
    "\n",
    "            # state ì£¼ì…\n",
    "            if use_state_ckpt:\n",
    "                robot_tensor = torch.from_numpy(np.asarray(robot_state[:state_dim], dtype=np.float32)).cuda()\n",
    "            else:\n",
    "                robot_tensor = torch.zeros(state_dim, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "            # ì´ë¯¸ì§€ í…ì„œ\n",
    "            image_tensor = get_image(ts, camera_names)\n",
    "\n",
    "            # ë°°ì¹˜\n",
    "            batch = policy.process_batch_to_llava(image_tensor, robot_tensor, raw_lang)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = policy.policy(**batch, eval=True)             # (1,T,D) or (T,D)\n",
    "                a_norm = out[0][0] if out.dim() == 3 else out[0]    # ì²« í† í°\n",
    "                a = denorm_action(a_norm) if head_type == \"droid_diffusion\" else a_norm\n",
    "                a_np = a.detach().cpu().numpy()\n",
    "\n",
    "                # === 10D -> ì ˆëŒ€ ëª©í‘œ (pos3 + rot6d + grip1) ===\n",
    "                if ckpt_action_dim == 10:\n",
    "                    pos_t   = a_np[:3].astype(np.float32)\n",
    "                    rot6d_t = torch.from_numpy(a_np[3:9]).to(\"cuda\", dtype=torch.float32).unsqueeze(0)\n",
    "                    R_t     = rot6d_to_matrix(rot6d_t)         # (1,3,3)\n",
    "                    q_t_xyzw = matrix_to_quat_xyzw(R_t)[0]     # (x,y,z,w)\n",
    "\n",
    "                    grip_t  = float(a_np[9])\n",
    "\n",
    "                    # í˜„ì¬ EEF (robosuiteëŠ” ê´€ì¸¡ì„ ë³´í†µ xyzwë¡œ ì¤€ë‹¤)\n",
    "                    curr_pos  = np.asarray(deploy_env.obs['robot0_eef_pos'],  dtype=np.float32)   # (3,)\n",
    "                    curr_quat_xyzw = np.asarray(deploy_env.obs['robot0_eef_quat'], dtype=np.float32)  # (x,y,z,w)\n",
    "\n",
    "                    # === ìœ„ì¹˜/ìì„¸ ë¸íƒ€ ===\n",
    "                    delta_pos = (pos_t - curr_pos) * POS_GAIN\n",
    "\n",
    "                    # q_delta = q_target âŠ— inv(q_curr)   (ëª¨ë‘ xyzw)\n",
    "                    q_delta_xyzw = quat_multiply(q_t_xyzw, quat_inverse(curr_quat_xyzw))\n",
    "                    axis_delta = quat2axisangle(q_delta_xyzw) * ROT_GAIN  # (3,)\n",
    "\n",
    "                    # === ê·¸ë¦¬í¼: ì ˆëŒ€ -> ì†ë„/ë¸íƒ€ ë³€í™˜ (EMAë¡œ ì•ˆì •í™”) ===\n",
    "                    if 'robot0_gripper_qpos' in deploy_env.obs:\n",
    "                        gq = np.asarray(deploy_env.obs['robot0_gripper_qpos'], dtype=np.float32)\n",
    "                        curr_open_frac = float(np.clip(np.mean(gq) / 0.04, 0.0, 1.0))\n",
    "                    else:\n",
    "                        curr_open_frac = 0.0\n",
    "\n",
    "                    # EMA ì—…ë°ì´íŠ¸ (ë¡œì»¬ ë³€ìˆ˜)\n",
    "                    if grip_ema is None:\n",
    "                        grip_ema = grip_t\n",
    "                    else:\n",
    "                        grip_ema = (1.0 - GRIP_EMA_ALPHA) * grip_ema + GRIP_EMA_ALPHA * grip_t\n",
    "\n",
    "                    # íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ìŠ¤ëƒ…\n",
    "                    if abs(grip_ema - 1.0) < 0.05:\n",
    "                        grip_ema = 1.0\n",
    "                    elif abs(grip_ema - 0.0) < 0.05:\n",
    "                        grip_ema = 0.0\n",
    "\n",
    "                    grip_vel = np.clip((grip_ema - curr_open_frac) * G_GAIN, -GRIP_VEL_CLIP, GRIP_VEL_CLIP)\n",
    "\n",
    "                    # === ìµœì¢… ì•¡ì…˜ ===\n",
    "                    action = np.zeros(7, dtype=np.float32)\n",
    "                    action[:3] = np.clip(delta_pos, -POS_CLIP, POS_CLIP)\n",
    "                    action[3:6] = np.clip(axis_delta, -ROT_CLIP, ROT_CLIP)\n",
    "                    action[6]   = float(grip_vel)\n",
    "                else:\n",
    "                    # 7D ì§ì ‘ ì¶œë ¥ì¼ ë•Œ(ë³´í†µ ë¸íƒ€ë¡œ ê°€ì •) â€” í•„ìš” ì‹œ ê²Œì¸/í´ë¦½\n",
    "                    action = a_np.astype(np.float32)\n",
    "                    action[:3] *= POS_GAIN\n",
    "                    action[3:6] *= ROT_GAIN\n",
    "                    action[:3] = np.clip(action[:3], -POS_CLIP, POS_CLIP)\n",
    "                    action[3:6] = np.clip(action[3:6], -ROT_CLIP, ROT_CLIP)\n",
    "                    action[6]   = np.clip(action[6], -1.0, 1.0)\n",
    "\n",
    "                if t < 6:\n",
    "                    print(f\"[Step {t:03d}]\")\n",
    "                    print(\"  a_norm:\", np.round(a_norm.detach().cpu().numpy(), 3))\n",
    "                    print(\"  a_denorm:\", np.round(a_np, 3))\n",
    "                    if ckpt_action_dim == 10:\n",
    "                        print(\"  target pos:\", np.round(pos_t, 3))\n",
    "                        print(\"  curr pos:\", np.round(curr_pos, 3))\n",
    "                        print(\"  delta_pos(gain):\", np.round(action[:3], 3))\n",
    "                        print(\"  axis_delta(gain):\", np.round(action[3:6], 3))\n",
    "                        print(\"  grip_vel:\", round(action[6], 3))\n",
    "\n",
    "\n",
    "                _, reward, done, _ = deploy_env.step(action)\n",
    "                print(f\"Step {t:03d} | Action: {np.round(action, 3)}\")\n",
    "\n",
    "                frames = deploy_env.render_cameras(cameras=camera_names, width=640, height=480)\n",
    "                if len(frames) == 3:\n",
    "                    try:\n",
    "                        frame_concat = np.concatenate(frames, axis=1)\n",
    "                        all_frames.append(frame_concat)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ğŸš¨ í”„ë ˆì„ ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    if save_episode and all_frames:\n",
    "        video_path = \"rollout_3_views_5.mp4\"\n",
    "        imageio.mimsave(video_path, all_frames, fps=fps)\n",
    "        print(f\"ğŸ¥ {video_path} ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4cf9eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: /home/parkjeongsu/.local/lib/python3.10/site-packages/robosuite/controllers/config/robots/default_panda.json (composite_controller_factory.py:121)\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load llaVA-Pythia MLLM!!!\n",
      "combine layer: Linear(in_features=519, out_features=512, bias=True)\n",
      "number of parameters: 7.283150e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/parkjeongsu/TinyVLA/OUTPUT_llava_pythia_4/checkpoint-20000 were not used when initializing LlavaPythiaForCausalLM: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlavaPythiaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaPythiaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device_map': 'cuda', 'torch_dtype': torch.float32}\n",
      "[ckpt] action_dim=10, state_dim=7, use_state=True, head=droid_diffusion\n",
      "[Step 000]\n",
      "  a_norm: [ 0.228  0.77   0.257 -0.635 -0.33  -0.208  0.885  0.143  0.368 -0.426]\n",
      "  a_denorm: [ 0.493  0.246  0.359 -0.198 -0.324 -0.314  0.885 -0.108  0.161  0.287]\n",
      "  target pos: [0.493 0.246 0.359]\n",
      "  curr pos: [-0.106  0.006  1.01 ]\n",
      "  delta_pos(gain): [ 0.03  0.03 -0.03]\n",
      "  axis_delta(gain): [ 0.159 -0.25   0.25 ]\n",
      "  grip_vel: 0.5\n",
      "Step 000 | Action: [ 0.03   0.03  -0.03   0.159 -0.25   0.25   0.5  ]\n",
      "[Step 001]\n",
      "  a_norm: [-0.501  0.815 -0.829 -0.441 -0.264 -0.874 -0.128  0.537  0.811  0.013]\n",
      "  a_denorm: [ 0.2    0.264 -0.07  -0.056 -0.259 -0.832 -0.128  0.2    0.369  0.506]\n",
      "  target pos: [ 0.2    0.264 -0.07 ]\n",
      "  curr pos: [-0.103  0.013  1.012]\n",
      "  delta_pos(gain): [ 0.03  0.03 -0.03]\n",
      "  axis_delta(gain): [-0.25 -0.25 -0.25]\n",
      "  grip_vel: 0.5\n",
      "Step 001 | Action: [ 0.03  0.03 -0.03 -0.25 -0.25 -0.25  0.5 ]\n",
      "[Step 002]\n",
      "  a_norm: [-0.894 -0.05  -0.388 -0.52  -0.725 -0.709  0.919  0.824  0.61  -0.851]\n",
      "  a_denorm: [ 0.043 -0.092  0.104 -0.114 -0.717 -0.704  0.92   0.424  0.274  0.075]\n",
      "  target pos: [ 0.043 -0.092  0.104]\n",
      "  curr pos: [-0.1    0.007  1.014]\n",
      "  delta_pos(gain): [ 0.028 -0.02  -0.03 ]\n",
      "  axis_delta(gain): [ 0.136 -0.25   0.25 ]\n",
      "  grip_vel: 0.5\n",
      "Step 002 | Action: [ 0.028 -0.02  -0.03   0.136 -0.25   0.25   0.5  ]\n",
      "[Step 003]\n",
      "  a_norm: [-0.744  0.176 -0.132 -0.479 -0.411 -0.704  0.797  0.64   0.731 -0.839]\n",
      "  a_denorm: [ 0.103  0.001  0.205 -0.084 -0.405 -0.7    0.797  0.281  0.331  0.08 ]\n",
      "  target pos: [0.103 0.001 0.205]\n",
      "  curr pos: [-0.097  0.007  1.015]\n",
      "  delta_pos(gain): [ 0.03  -0.001 -0.03 ]\n",
      "  axis_delta(gain): [ 0.216 -0.25   0.25 ]\n",
      "  grip_vel: 0.472\n",
      "Step 003 | Action: [ 0.03  -0.001 -0.03   0.216 -0.25   0.25   0.472]\n",
      "[Step 004]\n",
      "  a_norm: [-0.77  -0.243  0.562 -0.617 -0.844 -0.811  0.463 -0.517 -0.021 -0.84 ]\n",
      "  a_denorm: [ 0.092 -0.171  0.48  -0.185 -0.835 -0.784  0.463 -0.623 -0.021  0.08 ]\n",
      "  target pos: [ 0.092 -0.171  0.48 ]\n",
      "  curr pos: [-0.095  0.019  1.017]\n",
      "  delta_pos(gain): [ 0.03 -0.03 -0.03]\n",
      "  axis_delta(gain): [ 0.222 -0.25   0.25 ]\n",
      "  grip_vel: 0.4\n",
      "Step 004 | Action: [ 0.03  -0.03  -0.03   0.222 -0.25   0.25   0.4  ]\n",
      "[Step 005]\n",
      "  a_norm: [ 0.163  0.712  0.299 -0.552 -0.23  -0.861  0.625  0.707  0.762 -0.605]\n",
      "  a_denorm: [ 0.467  0.222  0.376 -0.138 -0.225 -0.822  0.625  0.333  0.346  0.198]\n",
      "  target pos: [0.467 0.222 0.376]\n",
      "  curr pos: [-0.092  0.032  1.019]\n",
      "  delta_pos(gain): [ 0.03  0.03 -0.03]\n",
      "  axis_delta(gain): [ 0.248 -0.25   0.25 ]\n",
      "  grip_vel: 0.397\n",
      "Step 005 | Action: [ 0.03   0.03  -0.03   0.248 -0.25   0.25   0.397]\n",
      "Step 006 | Action: [ 0.03   0.015 -0.03   0.177 -0.25   0.25   0.4  ]\n",
      "Step 007 | Action: [ 0.03   0.003 -0.03   0.033 -0.25  -0.25   0.407]\n",
      "Step 008 | Action: [ 0.03   0.03  -0.03   0.097 -0.25  -0.25   0.471]\n",
      "Step 009 | Action: [ 0.03   0.03  -0.03  -0.25  -0.144 -0.25   0.482]\n",
      "Step 010 | Action: [ 0.03  -0.03  -0.03   0.25  -0.081  0.25   0.413]\n",
      "Step 011 | Action: [ 0.025 -0.03  -0.03   0.054 -0.25   0.25   0.36 ]\n",
      "Step 012 | Action: [ 0.027  0.03  -0.03  -0.25  -0.25  -0.25   0.401]\n",
      "Step 013 | Action: [ 0.022 -0.004 -0.03   0.241 -0.25   0.25   0.349]\n",
      "Step 014 | Action: [ 0.03   0.03  -0.03   0.13  -0.25   0.25   0.406]\n",
      "Step 015 | Action: [ 0.03   0.03  -0.03   0.202 -0.25   0.131  0.375]\n",
      "Step 016 | Action: [ 0.021  0.03  -0.03   0.103 -0.25  -0.25   0.385]\n",
      "Step 017 | Action: [ 0.03  -0.013 -0.03  -0.087 -0.25  -0.23   0.384]\n",
      "Step 018 | Action: [ 0.03   0.003 -0.03   0.25  -0.25   0.25   0.4  ]\n",
      "Step 019 | Action: [ 0.03  -0.03  -0.03   0.081 -0.25   0.25   0.488]\n",
      "Step 020 | Action: [ 0.03   0.03  -0.03  -0.25  -0.25  -0.229  0.44 ]\n",
      "Step 021 | Action: [ 0.03  -0.024 -0.03   0.25  -0.25   0.25   0.39 ]\n",
      "Step 022 | Action: [ 0.03   0.03  -0.03   0.109 -0.25   0.25   0.383]\n",
      "Step 023 | Action: [ 0.03   0.03  -0.03   0.071 -0.25   0.25   0.405]\n",
      "Step 024 | Action: [ 0.03   0.03  -0.03   0.111 -0.25   0.25   0.375]\n",
      "Step 025 | Action: [ 0.009 -0.002 -0.03  -0.25  -0.25  -0.25   0.306]\n",
      "Step 026 | Action: [ 0.021 -0.03  -0.03   0.013 -0.25   0.25   0.444]\n",
      "Step 027 | Action: [ 0.03  -0.03  -0.03  -0.033 -0.25  -0.25   0.458]\n",
      "Step 028 | Action: [ 0.03   0.03  -0.03   0.25  -0.25  -0.165  0.397]\n",
      "Step 029 | Action: [ 0.03  -0.03  -0.03   0.163 -0.25   0.25   0.492]\n",
      "Step 030 | Action: [ 0.012 -0.002 -0.03  -0.175 -0.121 -0.25   0.5  ]\n",
      "Step 031 | Action: [ 0.03   0.03  -0.03   0.182 -0.25   0.25   0.5  ]\n",
      "Step 032 | Action: [ 0.03  -0.03  -0.03   0.19  -0.25  -0.25   0.459]\n",
      "Step 033 | Action: [ 0.029 -0.03  -0.03   0.081 -0.25   0.198  0.477]\n",
      "Step 034 | Action: [ 0.016 -0.019 -0.03   0.25  -0.25   0.25   0.5  ]\n",
      "Step 035 | Action: [ 0.018  0.03  -0.03   0.06  -0.25   0.25   0.5  ]\n",
      "Step 036 | Action: [ 0.03   0.028 -0.03   0.102 -0.154  0.25   0.494]\n",
      "Step 037 | Action: [ 0.029  0.005 -0.03   0.25  -0.25  -0.25   0.443]\n",
      "Step 038 | Action: [ 0.03  -0.015 -0.03   0.081 -0.25   0.228  0.448]\n",
      "Step 039 | Action: [ 0.03   0.03  -0.03   0.25  -0.25   0.141  0.469]\n",
      "Step 040 | Action: [ 0.03   0.03  -0.03   0.25  -0.25   0.101  0.5  ]\n",
      "Step 041 | Action: [ 0.009  0.03  -0.03   0.115  0.013  0.25   0.457]\n",
      "Step 042 | Action: [ 0.03  0.03 -0.03  0.25 -0.25 -0.25  0.4 ]\n",
      "Step 043 | Action: [ 0.006  0.03  -0.03   0.1   -0.25   0.202  0.421]\n",
      "Step 044 | Action: [ 0.007 -0.03  -0.03  -0.106 -0.25  -0.25   0.413]\n",
      "Step 045 | Action: [ 0.001  0.022 -0.03   0.217 -0.207 -0.25   0.389]\n",
      "Step 046 | Action: [ 0.003  0.03  -0.03   0.25  -0.25   0.25   0.496]\n",
      "Step 047 | Action: [ 0.03  -0.014 -0.03   0.008 -0.25  -0.25   0.445]\n",
      "Step 048 | Action: [ 0.03  -0.03  -0.03   0.048 -0.25   0.25   0.43 ]\n",
      "Step 049 | Action: [-0.008 -0.021 -0.03  -0.028 -0.25  -0.087  0.5  ]\n",
      "Step 050 | Action: [ 0.03  0.03 -0.03  0.1  -0.25  0.25  0.5 ]\n",
      "Step 051 | Action: [-0.011  0.03  -0.03  -0.06  -0.25   0.25   0.5  ]\n",
      "Step 052 | Action: [ 0.022 -0.004 -0.03   0.186  0.026  0.25   0.5  ]\n",
      "Step 053 | Action: [ 0.008  0.03  -0.03   0.089 -0.197  0.25   0.5  ]\n",
      "Step 054 | Action: [ 0.03  0.03 -0.03  0.25 -0.25  0.25  0.5 ]\n",
      "Step 055 | Action: [ 0.022  0.002 -0.03   0.175 -0.25  -0.111  0.5  ]\n",
      "Step 056 | Action: [ 0.015  0.03  -0.03   0.25  -0.25   0.25   0.5  ]\n",
      "Step 057 | Action: [ 0.019  0.03  -0.03   0.146 -0.25   0.25   0.5  ]\n",
      "Step 058 | Action: [-0.002  0.03  -0.03   0.168 -0.25   0.25   0.5  ]\n",
      "Step 059 | Action: [-0.011 -0.03  -0.03   0.25  -0.228  0.25   0.5  ]\n",
      "Step 060 | Action: [ 0.03 -0.02 -0.03  0.25 -0.25  0.25  0.5 ]\n",
      "Step 061 | Action: [ 0.029  0.025 -0.03  -0.125 -0.25  -0.25   0.5  ]\n",
      "Step 062 | Action: [-0.02   0.029 -0.03  -0.25  -0.25  -0.25   0.5  ]\n",
      "Step 063 | Action: [ 0.03   0.03  -0.03   0.06  -0.024  0.25   0.5  ]\n",
      "Step 064 | Action: [ 0.022  0.03  -0.03  -0.25  -0.25  -0.172  0.496]\n",
      "Step 065 | Action: [-0.019  0.03  -0.03   0.144 -0.25   0.25   0.485]\n",
      "Step 066 | Action: [-0.014  0.004 -0.03   0.25  -0.25   0.25   0.5  ]\n",
      "Step 067 | Action: [-0.013 -0.03  -0.03   0.045 -0.149  0.25   0.5  ]\n",
      "Step 068 | Action: [-0.003  0.03  -0.03   0.026 -0.25   0.25   0.5  ]\n",
      "Step 069 | Action: [ 0.03   0.03  -0.03   0.063 -0.25   0.25   0.5  ]\n",
      "Step 070 | Action: [-0.021 -0.018 -0.03   0.06  -0.25  -0.25   0.447]\n",
      "Step 071 | Action: [-0.005  0.007 -0.03  -0.11  -0.25  -0.25   0.384]\n",
      "Step 072 | Action: [ 0.03   0.004 -0.03   0.11  -0.25   0.25   0.396]\n",
      "Step 073 | Action: [ 0.03   0.03  -0.03   0.25  -0.25   0.25   0.437]\n",
      "Step 074 | Action: [ 0.023 -0.005 -0.03  -0.021 -0.036 -0.25   0.44 ]\n",
      "Step 075 | Action: [ 0.03  -0.001 -0.03   0.25  -0.25   0.25   0.5  ]\n",
      "Step 076 | Action: [ 0.014 -0.02  -0.03  -0.002 -0.25  -0.25   0.494]\n",
      "Step 077 | Action: [ 0.03   0.006 -0.03   0.07  -0.25  -0.25   0.43 ]\n",
      "Step 078 | Action: [-0.02   0.027 -0.03  -0.026 -0.25   0.25   0.5  ]\n",
      "Step 079 | Action: [ 0.03  -0.03  -0.03   0.107 -0.25   0.245  0.488]\n",
      "Step 080 | Action: [ 0.03  -0.03  -0.03  -0.136 -0.25   0.25   0.5  ]\n",
      "Step 081 | Action: [ 0.03   0.001 -0.03   0.071 -0.179  0.25   0.499]\n",
      "Step 082 | Action: [-0.019  0.03  -0.03   0.01  -0.25  -0.25   0.5  ]\n",
      "Step 083 | Action: [-0.008 -0.03  -0.03   0.111 -0.25   0.25   0.463]\n",
      "Step 084 | Action: [-0.022  0.023 -0.03  -0.08  -0.25  -0.25   0.461]\n",
      "Step 085 | Action: [-0.025  0.03  -0.03   0.073 -0.25   0.25   0.416]\n",
      "Step 086 | Action: [ 0.023 -0.03  -0.03   0.08  -0.25   0.25   0.379]\n",
      "Step 087 | Action: [ 0.03   0.03  -0.03  -0.014 -0.138  0.25   0.498]\n",
      "Step 088 | Action: [ 0.01   0.022 -0.03   0.087 -0.25  -0.25   0.465]\n",
      "Step 089 | Action: [-0.023 -0.022 -0.03   0.053 -0.25   0.25   0.464]\n",
      "Step 090 | Action: [-0.024  0.024 -0.03  -0.037 -0.25   0.25   0.466]\n",
      "Step 091 | Action: [-0.005 -0.021 -0.03  -0.199 -0.25  -0.25   0.41 ]\n",
      "Step 092 | Action: [-0.021 -0.03  -0.03  -0.028 -0.25   0.25   0.48 ]\n",
      "Step 093 | Action: [ 0.03  -0.03  -0.03   0.142 -0.25   0.25   0.46 ]\n",
      "Step 094 | Action: [ 0.015  0.001 -0.03  -0.081 -0.23   0.25   0.5  ]\n",
      "Step 095 | Action: [-0.023  0.016 -0.03  -0.034 -0.212  0.25   0.5  ]\n",
      "Step 096 | Action: [-0.002 -0.013 -0.03   0.167 -0.25   0.25   0.435]\n",
      "Step 097 | Action: [ 0.017 -0.004 -0.03   0.179 -0.25  -0.25   0.458]\n",
      "Step 098 | Action: [ 0.03   0.012 -0.03  -0.065 -0.162  0.25   0.408]\n",
      "Step 099 | Action: [-0.026 -0.03  -0.03  -0.037 -0.25  -0.055  0.485]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parkjeongsu/.local/lib/python3.10/site-packages/imageio_ffmpeg/_utils.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ rollout_3_views_5.mp4 ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ===================== ì‹¤í–‰ë¶€ =====================\n",
    "if __name__ == '__main__':\n",
    "    policy_config = {\n",
    "        \"model_path\": \"/home/parkjeongsu/TinyVLA/OUTPUT_llava_pythia_4/checkpoint-20000\",\n",
    "        \"model_base\": None,\n",
    "        \"enable_lora\": False,\n",
    "        \"conv_mode\": \"pythia\",\n",
    "        \"action_head\": \"droid_diffusion\",\n",
    "        \"action_dim\": 10,   # â† ckptì— ë§ê²Œ! (ë¡œê·¸ì—ì„œ 10 í™•ì¸ë¨)\n",
    "        \"chunk_size\": 10\n",
    "    }\n",
    "\n",
    "    LOGICAL_CAMERA_NAMES = (\"sideview\", \"sideview_opposite\", \"robot0_eye_in_hand\")\n",
    "    env = RobosuiteDeployEnv(env_name=\"Lift\", cameras=LOGICAL_CAMERA_NAMES)\n",
    "    policy = llava_pythia_act_policy(policy_config)\n",
    "\n",
    "    eval_bc(\n",
    "        policy, env, policy_config,\n",
    "        raw_lang=\"pick the cube up\",\n",
    "        n_steps=100, fps=10,\n",
    "        camera_names=LOGICAL_CAMERA_NAMES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42238d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinysuite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
